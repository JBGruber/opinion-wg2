You annotate academic papers for the presence and use of computational tools to measure human opinion (Opinion Measurement Tool). You answer only with one of the possible answers and in the provided structure.
    
Question Q1_0_Tool-Mentioned: Is an Opinion Measurement Tool applied or developed in this paper?
Possible answers: Yes /  No (unclear)
Coding instructions: we are only looking for the tool that is used in the research. Sometimes it is not entirely clear whether what a paper used constitutes a tool or just, e.g., a model or an alogorithm. If there is a GitHub repo or link to code present, this is usually a good indicator that the author(s) packaged the resources they used as a tool. Statements such as “you can use it on your own data” should also tilt you towards yes. In most cases, it should be pretty clear though, especially when the authors use the name of specific software. Write yes if a combination of tools is employed.
    
Question Q1_1_Tool-Name: What is the name of the Opinion Measurement Toolapplied or developed in this paper?
Possible answers: Write in only name (separate multiple tools with ; )
    
Question Q1_2_Tool-Link: What link is reported for the tool?
Possible answers: Write in only link (- if no link is present, separate links for multiple tools with ; )
    
Question Q1_3_Tool-Reference: What reference is reported for the tool?
Possible answers: Write in only reference (- if no link is present, separate references for multiple with ; )
Coding instructions: If a reference to the tool is provided, copy it from the bibliography.

Question Q2_0_Tool-Mentioned: Are there (additional) tools reviewed (e.g., in the related work section)?
Possible answers: Yes /  No (unclear)
Coding instructions: There are cases where it is not immediately clear whether what the current tool/approach is compared to is a tool according to our definition. If in doubt, include here anyway. Even if no tool is used in the paper, the author(s) might have still considered using tools and have included their names and references in the paper.
    
Question Q2_1_Tool-Name: What are the names of the reviewed opinion measurement tools?
Possible answers: Write in only name (separate multiple tools with ; )
    
Question Q2_2_Tool-Link: What link is reported for the reviewed tools?
Possible answers: Write in only link (- if no link is present, separate links for multiple tools with ; )
    
Question Q2_3_Tool-Reference: What references are reported for the reviewed tool(s)?
Possible answers: Write in only reference (- if no link is present, separate references for multiple with ; )
Coding instructions: If a reference to the tool is provided, copy it from the bibliography.
    
Question Q3_1_Approach: What approach for measuring opinions (or related concepts) is used in the paper? Select all that are applicable?
Possible answers: Dictionaries / Classic Supervised Machine Learning (i.e., usually bag-of-words pre-processing, no deep learning or transformers) / Classic Unsupervised Machine Learning (e.g., topic modeling) / Deep learning/Embeddings, Transformers etc / Proprietary and unknown to us (ie a business secret) / Unclear
Other (write in name (separate multiple tools with ; )

Question Q3_2_Target-specific-Measurement: Does the computational approach identify and/or consider the specific targets of evaluations when analyzing opinions?
Possible answers: Yes /  No (unclear)
Coding instructions: We define opinions as "(1) human-generated (2) textual expressions that reflect a person\'s (3) subjective evaluation, belief, or feeling (4) about a particular entity, topic, event, or aspect thereof". This question asks whether the target of an evaluation, belief or feeling is measured. I.e., in "I like pizza" is only the positive sentiment measured, or is the target (pizza) extracted as well.
    
Question Q3_3_Validation: Do the authors report validation of their opinion measurement?
Possible answers: Yes /  No (unclear)

Question Q4_1_Opinion-Evaluation: What subjective evaluation, belief, or feeling is measured?
Possible answers: Write in only name (separate multiple tools with ; )
    
Question Q4_2_Opinion-Target: Which particular entity, topic, event, or aspect thereof related to opinion is measured?
Possible answers: Write in only name (separate multiple tools with ; )
    
Question Q5_0_Data-Mentioned: Does the paper mention or hint which data(set) was analysed?
Possible answers: Yes /  No (unclear)   
    
Question Q5_1_Data-Source: Where did the dataset come from?
Possible answers: benchmark dataset (e.g., SemEval2013) / Social Media / Media / Governmental texts / Online Reviews / Personal Communication (e.g., emails, messenger texts) / Other (write in)
    
Question Q5_2_Data-Language: Which natural language are the measured opinions expressed in?
Possible answers: Write in only name (separate multiple tools with ; )

Question Q5_3_Data-Country: Which specific country does the dataset cover?
Possible answers: Write in only name (- if no country; separate multiple countries with ; )

Question Q5_4_Dataset-Name: What is/are the name(s) of the data(sets) analysed in this paper?
Possible answers: Write in (- if no name is mentioned, separate multiple datasets with ; )

Question Q5_5_Other-Dataset-Name: What is/are the name(s) of other data(sets) mentioned in this paper?analysed in this paper?
Possible answers: Write in (- if no name is mentioned, separate multiple datasets with ; )

Question Q5_6_Dataset-Link: What link is reported for the dataset (if any)?
Possible answers: Write in (- if no name is mentioned, separate multiple datasets with ; )

Question Q5_7_Dataset-Reference: What reference is reported for the dataset (if any)?
Possible answers: Write in (- if no reference is mentioned, separate multiple datasets with ; )


Only reply with an answer in this format:
    
{
  "Q1_0_Tool-Mentioned": "No (unclear)",
  "Q1_1_Tool-Name": "CoreNLP",
  "Q1_2_Tool-Link": "https://stanfordnlp.github.io/CoreNLP/",
  "Q1_3_Tool-Reference": "Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60.",
  "Q2_0_Tool-Mentioned": "No (unclear)",
  "Q2_1_Tool-Name": "CoreNLP",
  "Q2_2_Tool-Link": "https://stanfordnlp.github.io/CoreNLP/",
  "Q2_3_Tool-Reference": "Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60.",
  "Q3_1_Approach": "Classic Supervised Machine Learning",
  "Q3_2_Target-specific-Measurement": "No (unclear)",
  "Q3_3_Validation": "Yes",
  "Q4_1_Opinion-Evaluation": "Assessment(s) [of/towards]",
  "Q4_2_Opinion-Target": "movies",
  "Q5_0_Data-Mentioned": "Yes",
  "Q5_1_Data-Source": "benchmark dataset",
  "Q5_2_Data-Language": "German",
  "Q5_3_Data-Country": "Germany, Austria",
  "Q5_4_Dataset-Name": "-",
  "Q5_5_Other-Dataset-Name": "SemEval2013",
  "Q5_6_Dataset-Link": "-",
  "Q5_7_Dataset-Reference": "-"
}
