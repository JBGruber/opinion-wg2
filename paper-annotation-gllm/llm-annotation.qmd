---
title: "PDF annotation"
format: html
---

```{r}
if (!reticulate::virtualenv_exists("r-opinion")) {
  reticulate::virtualenv_create("r-opinion")
  reticulate::virtualenv_install("r-opinion", c(
    "openparse",
    "pypdf",
    "langchain_community",
    "pyautogui",
    "pygetwindow"
  ))
}
reticulate::use_virtualenv("r-opinion")
library(reticulate)
library(tidyverse)
library(rollama)
```

# test data

```{r}
test_data <- rio::import("https://github.com/JBGruber/opinion-wg2/raw/main/paper-annotation/4._annotations_jbg.xlsx") |> 
  select(unit_id, variable, Final, file, url)
```

This works pretty well with complex PDFs, as it breaks up the content into boxes first and then reads these boxes (which are usually paragraphs).
Unfortunately, the package has a bug that omit all whitespace in some PDFs.

```{python}
import openparse
def pdf2df(path):
    doc = openparse.DocumentParser().parse(path)
    df = {"page": [], "text": [], "source": path}
    for node in doc.nodes:
        df["text"].append(node.text)
        df["page"].append(node.start_page)
    return df
```

Alternatively, we can use the same approach as [langchain](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/).

```{python}
from pypdf import PdfReader
def pdf2df2(path):
    doc = PdfReader(path)
    df = {"page": [], "text": [], "source": path}
    for page in doc.pages:
        df["text"].append(page.extract_text())
        df["page"].append(page.page_number)
    return df
```

The default package in R for PDF text extraction is `pdftools`, which I think works essentially just as well.

```{r}
pdf2df3 <- function(path) {
  tibble(
    text = pdftools::pdf_text(path),
    page = seq_along(text),
    source = path
  )
}
```

# RAG functions

Just the function definitions

```{r}
# parses PDFs into data.frames
pdf2df <- function(path, v = "1") {
  cont <- switch (v,
    "1" = py$pdf2df(path),
    "2" = py$pdf2df2(path),
    "3" = pdf2df3(path)
  )
  
  if (length(cont$text) > 2L) {
    return(as_tibble(cont))
  } else {
    warning("No content returned")
    return(tibble())
  }
}

# finds the n texts in haystack (vector db) that are most similar to needle (string)
find_most_similar <- function(needle, haystack, n = 5) {
  
  needle_mbd <- embed_text(needle, model = haystack$model[1], verbose = FALSE) |> 
    as.matrix()
  
  haystack_mbd <- haystack |> 
    select(starts_with("dim_")) |> 
    as.matrix()
  
  haystack_content <- haystack |> 
    select(!starts_with("dim_"))
  
  # TODO: which one is faster on larger dbs
  # most_similar <- proxyC::simil(haystack_mbd, needle_mbd, rank = n) |> 
  #   order(decreasing = TRUE) |> 
  #   head(n)
  
  most_similar <- Matrix::tcrossprod(needle_mbd, haystack_mbd) |> 
    order(decreasing = TRUE) |> 
    head(n)
  
  haystack_content |> 
    slice(most_similar)
}

# query with RAG support
query_rag <- function(prompt, context_db, model = "mistral") {
  
  context <- find_most_similar(prompt, context_db) |> 
    pull(text) |> 
    paste(collapse = "\n\n")
  
  q <- tribble(
    ~role,    ~content,
    "system", glue::glue("You are a helpful reading assistant who answers questions based on snippets of text provided in context. Answer only using the context provided, being as concise as possible. If you're unsure, just say that you don't know.

Context:
  
  {context}"),
    "user", prompt
  )
  
  query(q, model = model)
}
```

Let's test the PDF parsers:

```{r}
pdf2df("/mnt/dropbox/Dropbox/opinion_pdfs/10.1007_978-3-030-00072-1_13.pdf", v = "3")
```


# create vector representation database

```{r}
test_file <- test_data |> 
  head(1)
if (!file.exists(test_data$file[1])) {
  download.file(test_data$url[1],
                test_data$file[1])
}

context_db <- pdf2df(test_data$file[1], v = "2") |> 
  mutate(model = "all-minilm",
         embeddings = embed_text(text, model = "all-minilm")) |> 
  unnest_wider(embeddings)
```

A quick test:

```{r}
find_most_similar("Is a tool for opinion measurement applied or developed in this paper?", context_db)
```

# Annotate PDFs

```{r}
query_rag("Is a tool for opinion measurement applied or developed in this paper?", 
          context_db = context_db, 
          model = "llama3:8b")
```

```{r}
test_data |> 
  filter(unit_id == test_data$unit_id[1]) |> 
  filter(str_detect(variable, "Q1_"))
```

